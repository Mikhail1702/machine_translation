{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZWE7NAYygHT"
   },
   "source": [
    "<div style=\"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: center;\">\n",
    "    <img src = \"https://i.imgur.com/dVmXKjB.png\" width = 400, height= 400>\n",
    "    <div style=\"font-size: 42px; font-weight: bold; color: #121212; text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\">\n",
    "        <b>Transformer From Scratch With PyTorchðŸ”¥</b>\n",
    "    </div>\n",
    "    <hr style=\"border: none;\n",
    "               border-top: 1.25px solid #121212;\n",
    "               width: 60%;\n",
    "               margin-top: 20px;\n",
    "               margin-bottom: 20px;\n",
    "               margin-left: auto;\n",
    "               margin-right: auto;\">\n",
    "    <div style=\"font-weight: bold;\n",
    "                text-transform: uppercase;\n",
    "                letter-spacing: 1.5px;\n",
    "                color: #121212;\n",
    "                \">2024 | <a href =\"https://www.kaggle.com/lusfernandotorres/\" style=\"color: #121212; text-decoration: none;\">Â© Luis Fernando Torres</a></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMpWueJpygHU"
   },
   "source": [
    "<br><br><br>\n",
    "<div style=\"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 18px; letter-spacing: 1.5px; margin-top: 25px; margin-bottom: 25px;\">\n",
    "    Table of Contents\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "- [Introduction](#intro)<br><br>\n",
    "- [Transformer Architecture](#transformer)<br><br>\n",
    "    - [Input Embeddings](#embed)<br><br>\n",
    "    - [Positional Encoding](#positional-encoding)<br><br>\n",
    "    - [Layer Normalization](#layer-normalization)<br><br>\n",
    "    - [Feed-Forward Network](#feedforward)<br><br>\n",
    "    - [Multi-Head Attention](#multihead)<br><br>\n",
    "    - [Residual Connection](#residual-connection)<br><br>\n",
    "    - [Encoder](#encoder)<br><br>\n",
    "    - [Decoder](#decoder)<br><br>\n",
    "    - [Building the Transformer](#building-transformer)<br><br>\n",
    "- [Tokenizer](#tokenizer)<br><br>\n",
    "- [Loading Dataset](#dataset)<br><br>\n",
    "- [Validation Loop](#val)<br><br>\n",
    "- [Training Loop](#train)<br><br>\n",
    "- [Conclusion](#conclusion)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIG4Yk-xygHU"
   },
   "source": [
    "<div id = 'intro'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\">Introduction</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppCd4kLIygHU"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In 2017, the Google Research team published a paper called <a href = \"https://arxiv.org/pdf/1706.03762.pdf\"><i>\"Attention Is All You Need\"</i></a>, which presented the Transformer architecture and was a paradigm shift in Machine Learning, especially in Deep Learning and the field of natural language processing.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The Transformer, with its parallel processing capabilities, allowed for more efficient and scalable models, making it easier to train them on large datasets. It also demonstrated superior performance in several NLP tasks, such as sentiment analysis and text generation tasks.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The archicture presented in this paper served as the foundation for subsequent models like GPT and BERT. Besides NLP, the Transformer architecture is used in other fields, like audio processing and computer vision. You can see the usage of Transformers in audio classification in the notebook <a href=\"https://www.kaggle.com/code/lusfernandotorres/audio-data-music-genre-classification\"><b>Audio Data: Music Genre Classification.</b></a></p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Even though you can easily employ different types of Transformers with the <a href = \"https://huggingface.co/docs/transformers/index\"><b>ðŸ¤—Transformers</b></a> library, it is crucial to understand how things truly work by building them from scratch.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In this notebook, we will explore the Transformer architecture and all its components. I will use PyTorch to build all the necessary structures and blocks, and I will use the <a href = \"https://www.youtube.com/watch?v=ISNdQcPhsts&t=9595s\"><b>Coding a Transformer from scratch on PyTorch, with full explanation, training and inference</b></a> video posted by <a href = \"https://www.youtube.com/@umarjamilai\"><b>Umar Jamil</b></a> on YouTube as reference.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Let's start by importing all the necessary libraries.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "on_giWZ_qnr0",
    "outputId": "d266a834-2039-45ee-ded1-60e37caad975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\1\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0Ul47Mkyrh_",
    "outputId": "54e149bf-c915-430b-87f1-beb47ac431ae",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\1\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\1\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZaVUrHxqnr4",
    "outputId": "43784256-2e9f-4c0f-e5fa-f7096f44b888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from tokenizers) (0.22.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rFN_xYHtqnr4",
    "outputId": "a3e5a22b-8d31-4b32-993c-461240b7177b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\1\\.conda\\envs\\cude_env\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "54wQhg0HygHU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\.conda\\envs\\cude_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Math\n",
    "import math\n",
    "\n",
    "# HuggingFace libraries\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "# typing\n",
    "from typing import Any\n",
    "\n",
    "# Library for progress bars in loops\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing library of warnings\n",
    "import warnings\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import sacrebleu as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkqGdJdRygHV"
   },
   "source": [
    "<div id = 'transformer'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\"><br><br>Transformer Architecture</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RCDws5pygHV"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Before coding, let's take a look at the Transformer architecture.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6Qw_TvyygHV"
   },
   "source": [
    "<center>\n",
    "    <img src = \"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*s5XcjuosS8ohfsW5xFT3sQ.png\" width = 600, height= 600>\n",
    "<p style = \"font-size: 16px;\n",
    "            font-family: 'Georgia', serif;\n",
    "            text-align: center;\n",
    "            margin-top: 10px;\">Source: <a href = \"https://arxiv.org/pdf/1706.03762.pdf\">Attention Is All You Need</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWEoWHEPygHV"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The Transformer architecture has two main blocks: the <b>encoder</b> and the <b>decoder</b>. Let's take a look at them further.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>Encoder:</b> It has a <i>Multi-Head Attention</i> mechanism and a fully connected <i>Feed-Forward</i> network. There are also residual connections around the two sub-layers, plus layer normalization for the output of each sub-layer. All sub-layers in the model and the embedding layers produce outputs of dimension $d_{model} = 512$. </p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>Decoder:</b> The decoder follows a similar structure, but it inserts a third sub-layer that performs multi-head attention over the output of the encoder block. There is also a modification of the self-attention sub-layer in the decoder block to avoid positions from attending to subsequent positions. This masking ensures that the predictions for position $i$ depend solely on the known outputs at positions less than $i$.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Both the encoder and decode blocks are repeated $N$ times. In the original paper, they defined $N = 6$, and we will define a similar value in this notebook.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2v-L5sDLygHV"
   },
   "source": [
    "<div id = 'embed'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 32px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\">Input Embeddings</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHRlIlVOygHW"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">When we observe the Transformer architecture image above, we can see that the Embeddings represent the first step of both blocks.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>InputEmbedding</code> class below is responsible for converting the input text into numerical vectors of <code>d_model</code> dimensions. To prevent that our input embeddings become extremely small, we normalize them by multiplying them by the $\\sqrt{d_{model}}$.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the image below, we can see how the embeddings are created. First, we have a sentence that gets split into tokensâ€”we will explore what tokens are later onâ€”. Then, the token IDsâ€”identification numbersâ€”are transformed into the embeddings, which are high-dimensional vectors.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nVTuY0lygHW"
   },
   "source": [
    "<center>\n",
    "    <img src = \"https://vaclavkosar.com/images/transformer-tokenization-and-embeddings.drawio.svg\" width = 600, height= 600>\n",
    "<p style = \"font-size: 16px;\n",
    "            font-family: 'Georgia', serif;\n",
    "            text-align: center;\n",
    "            margin-top: 10px;\">Source: <a href = \"https://vaclavkosar.com/ml/Tokenization-in-Machine-Learning-Explained\">vaclavkosar.com</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x4fqOH9GtWzW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 20.2 s\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Path to dataset\n",
    "word2vec_path = \"model.bin\"\n",
    "\n",
    "# Load 200,000 most common words\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.044528, -0.096883,  0.224323, -0.49059 ,  0.103135, -0.489105,\n",
       "       -0.161784, -0.447556,  0.295135,  0.328013, -0.192237,  0.232075,\n",
       "        0.170114,  0.484106, -0.279735,  0.204341, -0.094503,  0.117511,\n",
       "        0.253401,  0.222023,  0.259212, -0.193704, -0.166642, -0.407859,\n",
       "       -0.250956, -0.253156, -0.054415,  0.00742 ,  0.237777, -0.21371 ,\n",
       "       -0.331639, -0.201904, -0.358056,  0.138375, -0.096008, -0.00319 ,\n",
       "        0.169703, -0.088066,  0.248336, -0.633195, -0.23755 ,  0.24702 ,\n",
       "       -0.303505,  0.034106,  0.084226,  0.558506,  0.478622,  0.170326,\n",
       "       -0.424445,  0.210645,  0.109966,  0.050654, -0.036247, -0.133888,\n",
       "       -0.331586,  0.154153,  0.313411, -0.081046,  0.10194 , -0.063442,\n",
       "       -0.520147, -0.232423, -0.547739,  0.195655, -0.066053, -0.026521,\n",
       "        0.186097,  0.191084,  0.247666, -0.296516, -0.110181,  0.206618,\n",
       "        0.120696, -0.354062,  0.216559, -0.099804, -0.360119,  0.31864 ,\n",
       "        0.210658,  0.026484,  0.163331,  0.236709, -0.056396,  0.016821,\n",
       "       -0.108485, -0.384459, -0.486444, -0.08825 ,  0.532944,  0.321581,\n",
       "        0.129812,  0.574755,  0.090896,  0.247661,  0.155368, -0.327642,\n",
       "       -0.266209,  0.230764,  0.057969,  0.119431], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model['you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CF8mQMGJqnr6"
   },
   "outputs": [],
   "source": [
    "# warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# config = get_config()\n",
    "# ds_raw = dict_dataset\n",
    "\n",
    "# # Building or loading tokenizer for both the source and target languages\n",
    "# tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "# tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "# # Processing data with the BilingualDataset class, which we will define below\n",
    "# ds = BilingualDataset(ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "# data = []\n",
    "# for i in ds:\n",
    "#     a = i['encoder_input']\n",
    "#     a = a[:list(np.array(a)).index(1)+5]\n",
    "#     data.append(list(np.array(a)))\n",
    "\n",
    "# # print(*data,sep='\\n')\n",
    "\n",
    "# model = gensim.models.Word2Vec(data, min_count=1, vector_size=512,\n",
    "#                                 window=5, sg=1, epochs=20)\n",
    "\n",
    "# model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vyY9giV_ygHW"
   },
   "outputs": [],
   "source": [
    "# Creating Input Embeddings\n",
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimension of vectors (512)\n",
    "        self.vocab_size = vocab_size # Size of the vocabulary\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # PyTorch layer that converts integer indices to dense embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            return x * math.sqrt(self.d_model) # Normalizing the variance of the embeddings\n",
    "        else:\n",
    "            return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5yGMnXtygHW"
   },
   "source": [
    "<div id = 'positional-encoding'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 32px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\">Positional Encoding</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71iERRWKygHW"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the original paper, the authors add the positional encodings to the input embeddings at the bottom of both the encoder and decoder blocks so the model can have some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two vectors can be summed and we can combine the semantic content from the word embeddings and positional information from the positional encodings.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>PositionalEncoding</code> class below, we will create a matrix of positional encodings <code>pe</code> with dimensions <code>(seq_len, d_model)</code>. We will start by filling it with $0$s.We will then apply the sine function to even indices of the positional encoding matrix while the cosine function is applied to the odd ones.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CFCXML3ygHW"
   },
   "source": [
    "<p style=\"\n",
    "    margin-bottom: 5;\n",
    "    font-size: 22px;\n",
    "    font-weight: 300;\n",
    "    font-family: 'Helvetica Neue', sans-serif;\n",
    "    color: #000000;\n",
    "  \">\n",
    "    \\begin{equation}\n",
    "    \\text{Even Indices } (2i): \\quad \\text{PE(pos, } 2i) = \\sin\\left(\\frac{\\text{pos}}{10000^{2i / d_{model}}}\\right)\n",
    "    \\end{equation}\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sKvwXmwygHW"
   },
   "source": [
    "<p style=\"\n",
    "    margin-bottom: 5;\n",
    "    font-size: 22px;\n",
    "    font-weight: 300;\n",
    "    font-family: 'Helvetica Neue', sans-serif;\n",
    "    color: #000000;\n",
    "  \">\n",
    "    \\begin{equation}\n",
    "    \\text{Odd Indices } (2i + 1): \\quad \\text{PE(pos, } 2i + 1) = \\cos\\left(\\frac{\\text{pos}}{10000^{2i / d_{model}}}\\right)\n",
    "    \\end{equation}\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi6p7GcUygHW"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We apply the sine and cosine functions because it allows the model to determine the position of a word based on the position of other words in the sequence, since for any fixed offset $k$, $PE_{pos + k}$ can be represented as a linear function of $PE_{pos}$. This happens due to the properties of sine and cosine functions, where a shift in the input results in a predictable change in the output.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GdYVOikAygHW"
   },
   "outputs": [],
   "source": [
    "# Creating the Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimensionality of the model\n",
    "        self.seq_len = seq_len # Maximum sequence length\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer to prevent overfitting\n",
    "\n",
    "        # Creating a positional encoding matrix of shape (seq_len, d_model) filled with zeros\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        # Creating a tensor representing positions (0 to seq_len - 1)\n",
    "        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # Transforming 'position' into a 2D tensor['seq_len, 1']\n",
    "\n",
    "        # Creating the division term for the positional encoding formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine to even indices in pe\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices in pe\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Adding an extra dimension at the beginning of pe matrix for batch handling\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Registering 'pe' as buffer. Buffer is a tensor not considered as a model parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Addind positional encoding to the input tensor X\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x) # Dropout for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dk9TlRA6ygHW"
   },
   "source": [
    "<div id = 'layer-normalization'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 32px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\">Layer Normalization</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufxEsYAOygHW"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">When we look at the encoder and decoder blocks, we see several normalization layers called <b><i>Add &amp; Norm</i></b>.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>LayerNormalization</code> class below performs layer normalization on the input data. During its forward pass, we compute the mean and standard deviation of the input data. We then normalize the input data by subtracting the mean and dividing by the standard deviation plus a small number called epsilon to avoid any divisions by zero. This process results in a normalized output with a mean 0 and a standard deviation 1.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will then scale the normalized output by a learnable parameter <code>alpha</code> and add a learnable parameter called <code>bias</code>. The training process is responsible for adjusting these parameters. The final result is a layer-normalized tensor, which ensures that the scale of the inputs to layers in the network is consistent.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "liz3NzbaygHX"
   },
   "outputs": [],
   "source": [
    "# Creating Layer Normalization\n",
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, eps: float = 10**-6) -> None: # We define epsilon as 0.000001 to avoid division by zero\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        # We define alpha as a trainable parameter and initialize it with ones\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # One-dimensional tensor that will be used to scale the input data\n",
    "\n",
    "        # We define bias as a trainable parameter and initialize it with zeros\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # One-dimensional tenso that will be added to the input data\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n",
    "        std = x.std(dim = -1, keepdim = True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
    "\n",
    "        # Returning the normalized input\n",
    "        return self.alpha * (x-mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-Gmi52GygHX"
   },
   "source": [
    "<div id = 'feedforward'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 32px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\">Feed-Forward Network</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPPFQmm6ygHX"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the fully connected feed-forward network, we apply two linear transformations with a ReLU activation in between. We can mathematically represent this operation as:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99dFSoeZygHX"
   },
   "source": [
    "<p style=\"\n",
    "    margin-bottom: 5;\n",
    "    font-size: 22px;\n",
    "    font-weight: 300;\n",
    "    font-family: 'Helvetica Neue', sans-serif;\n",
    "    color: #000000;\n",
    "  \">\n",
    "    \\begin{equation}\n",
    "    \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "    \\end{equation}\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ9N8oOHygHX"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">$W_1$ and $W_2$ are the weights, while $b_1$ and $b_2$ are the biases of the two linear transformations.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>FeedForwardBlock</code> below, we will define the two linear transformationsâ€”<code>self.linear_1</code> and <code>self.linear_2</code>â€”and the inner-layer <code>d_ff</code>. The input data will first pass through the <code>self.linear_1</code> transformation, which increases its dimensionality from <code>d_model</code> to <code>d_ff</code>. The output of this operation passes through the ReLU activation function, which introduces non-linearity so the network can learn more complex patterns, and the <code>self.dropout</code> layer is applied to mitigate overfitting. The final operation is the <code>self.linear_2</code> transformation to the dropout-modified tensor, which transforms it back to the original <code>d_model</code> dimension.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "77diakr2ygHX"
   },
   "outputs": [],
   "source": [
    "# Creating Feed Forward Layers\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # First linear transformation\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 & b1\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout to prevent overfitting\n",
    "        # Second linear transformation\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 & b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EESFLSs9ygHX"
   },
   "source": [
    "<div id = 'multihead'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 32px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\">Multi-Head Attention</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swkNaXfoygHX"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The Multi-Head Attention is the most crucial component of the Transformer. It is responsible for helping the model to understand complex relationships and patterns in the data.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The image below displays how the Multi-Head Attention works. It doesn't include <code>batch</code> dimension because it only illustrates the process for one single sentence.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GsAvD2aygHX"
   },
   "source": [
    "<center>\n",
    "    <img src = \"https://i.imgur.com/JqJVrsj.png\" width = 1556, height= 959>\n",
    "<p style = \"font-size: 16px;\n",
    "            font-family: 'Georgia', serif;\n",
    "            text-align: center;\n",
    "            margin-top: 10px;\">Source: <a href = \"https://www.youtube.com/watch?v=ISNdQcPhsts&t=9595s\">YouTube: Coding a Transformer from scratch on PyTorch, with full explanation, training and inference</a> by <a href = \"https://www.youtube.com/@umarjamilai\">Umar Jamil</a>.</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-fR0W1HygHX"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The Multi-Head Attention block receives the input data split into queries, keys, and values organized into matrices $Q$, $K$, and $V$. Each matrix contains different facets of the input, and they have the same dimensions as the input.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We then linearly transform each matrix by their respective weight matrices $W^Q$, $W^K$, and $W^V$. These transformations will result in new matrices $Q'$, $K'$, and $V'$, which will be split into smaller matrices corresponding to different heads $h$, allowing the model to attend to information from different representation subspaces in parallel. This split creates multiple sets of queries, keys, and values for each head.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Finally, we concatenate every head into an $H$ matrix, which is then transformed by another weight matrix $W^o$ to produce the multi-head attention output, a matrix $MH-A$ that retains the input dimensionality.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jbrcXxCkygHX"
   },
   "outputs": [],
   "source": [
    "# Creating the Multi-Head Attention block\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None: # h = number of heads\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        # We ensure that the dimensions of the model is divisible by the number of heads\n",
    "        assert d_model % h == 0, 'd_model is not divisible by h'\n",
    "\n",
    "        # d_k is the dimension of each attention head's key, query, and value vectors\n",
    "        self.d_k = d_model // h # d_k formula, like in the original \"Attention Is All You Need\" paper\n",
    "\n",
    "        # Defining the weight matrices\n",
    "        self.w_q = nn.Linear(d_model, d_model) # W_q\n",
    "        self.w_k = nn.Linear(d_model, d_model) # W_k\n",
    "        self.w_v = nn.Linear(d_model, d_model) # W_v\n",
    "        self.w_o = nn.Linear(d_model, d_model) # W_o\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer to avoid overfitting\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):# mask => When we want certain words to NOT interact with others, we \"hide\" them\n",
    "\n",
    "        d_k = query.shape[-1] # The last dimension of query, key, and value\n",
    "\n",
    "        # We calculate the Attention(Q,K,V) as in the formula in the image above\n",
    "        attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k) # @ = Matrix multiplication sign in PyTorch\n",
    "\n",
    "        # Before applying the softmax, we apply the mask to hide some interactions between words\n",
    "        if mask is not None: # If a mask IS defined...\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9) # Replace each value where mask is equal to 0 by -1e9\n",
    "        attention_scores = attention_scores.softmax(dim = -1) # Applying softmax\n",
    "        if dropout is not None: # If a dropout IS defined...\n",
    "            attention_scores = dropout(attention_scores) # We apply dropout to prevent overfitting\n",
    "\n",
    "        return (attention_scores @ value), attention_scores # Multiply the output matrix by the V matrix, as in the formula\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "\n",
    "        query = self.w_q(q) # Q' matrix\n",
    "        key = self.w_k(k) # K' matrix\n",
    "        value = self.w_v(v) # V' matrix\n",
    "\n",
    "\n",
    "        # Splitting results into smaller matrices for the different heads\n",
    "        # Splitting embeddings (third dimension) into h parts\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "\n",
    "        # Obtaining the output and the attention scores\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Obtaining the H matrix\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        return self.w_o(x) # Multiply the H matrix by the weight matrix W_o, resulting in the MH-A matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hhntihdygHY"
   },
   "source": [
    "<div id = 'residual-connection'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 32px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\">Residual Connection</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTDRnojQygHY"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">When we look at the architecture of the Transformer, we see that each sub-layer, including the <i>self-attention</i> and <i>Feed Forward</i> blocks, adds its output to its input before passing it to the <i>Add &amp; Norm</i> layer. This approach integrates the output with the original input in the <i>Add &amp; Norm</i> layer. This process is known as the skip connection, which allows the Transformer to train deep networks more effectively by providing a shortcut for the gradient to flow through during backpropagation.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>ResidualConnection</code> class below is responsible for this process.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cQzi4_uBygHY"
   },
   "outputs": [],
   "source": [
    "# Building Residual Connection\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout) # We use a dropout layer to prevent overfitting\n",
    "        self.norm = LayerNormalization() # We use a normalization layer\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # We normalize the input and add it to the original input 'x'. This creates the residual connection process.\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uc19BYIBygHY"
   },
   "source": [
    "<div id = 'encoder'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 32px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\">Encoder</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0-ovTjXygHY"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will now build the encoder. We create the <code>EncoderBlock</code> class, consisting of the Multi-Head Attention and Feed Forward layers, plus the residual connections.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX_B1jd1ygHY"
   },
   "source": [
    "<center>\n",
    "    <img src = \"https://www.researchgate.net/profile/Ehsan-Amjadian/publication/352239001/figure/fig1/AS:1033334390013952@1623377525434/Detailed-view-of-a-transformer-encoder-block-It-first-passes-the-input-through-an.jpg\" width = 400, height= 400>\n",
    "<p style = \"font-size: 16px;\n",
    "            font-family: 'Georgia', serif;\n",
    "            text-align: center;\n",
    "            margin-top: 10px;\">Encoder block. Source: <a href = \"https:///figure/Detailed-view-of-a-transformer-encoder-block-It-first-passes-the-input-through-an_fig1_352239001\">researchgate.net</a>.</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9d4F_AkygHY"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the original paper, the Encoder Block repeats six times. We create the <code>Encoder</code> class as an assembly of multiple <code>EncoderBlock</code>s. We also add layer normalization as a final step after processing the input through all its blocks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ITVDV9G8ygHY"
   },
   "outputs": [],
   "source": [
    "# Building Encoder Block\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    # This block takes in the MultiHeadAttentionBlock and FeedForwardBlock, as well as the dropout rate for the residual connections\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # Storing the self-attention block and feed-forward block\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # 2 Residual Connections with dropout\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # Applying the first residual connection with the self-attention block\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask)) # Three 'x's corresponding to query, key, and value inputs plus source mask\n",
    "\n",
    "        # Applying the second residual connection with the feed-forward block\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x # Output tensor after applying self-attention and feed-forward layers with residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "haHUe6M_ygHZ"
   },
   "outputs": [],
   "source": [
    "# Building Encoder\n",
    "# An Encoder can have several Encoder Blocks\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    # The Encoder takes in instances of 'EncoderBlock'\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers # Storing the EncoderBlocks\n",
    "        self.norm = LayerNormalization() # Layer for the normalization of the output of the encoder layers\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Iterating over each EncoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask) # Applying each EncoderBlock to the input tensor 'x'\n",
    "        return self.norm(x) # Normalizing output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahpI5nkEygHZ"
   },
   "source": [
    "<div id = 'decoder'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 32px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\">Decoder</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2PGX2O-ygHZ"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Similarly, the Decoder also consists of several DecoderBlocks that repeat six times in the original paper. The main difference is that it has an additional sub-layer that performs multi-head attention with a <i>cross-attention</i> component that uses the output of the Encoder as its keys and values while using the Decoder's input as queries.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwAiB9yCygHZ"
   },
   "source": [
    "<center>\n",
    "    <img src = \"https://res.cloudinary.com/edlitera/image/upload/c_fill,f_auto/v1680629118/blog/gz5ccspg3yvq4eo6xhrr\" width = 400, height= 400>\n",
    "<p style = \"font-size: 16px;\n",
    "            font-family: 'Georgia', serif;\n",
    "            text-align: center;\n",
    "            margin-top: 10px;\">Decoder block. Source: <a href = \"https://www.edlitera.com/blog/posts/transformers-decoder-block\">edlitera.com</a>.</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx6AfkojygHZ"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">For the Output Embedding, we can use the same <code>InputEmbeddings</code> class we use for the Encoder. You can also notice that the self-attention sub-layer is <i>masked</i>, which restricts the model from accessing future elements in the sequence.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will start by building the <code>DecoderBlock</code> class, and then we will build the <code>Decoder</code> class, which will assemble multiple <code>DecoderBlock</code>s.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PI31hfMHygHZ"
   },
   "outputs": [],
   "source": [
    "# Building Decoder Block\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    # The DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention, while the other is cross-attention.\n",
    "    # It also takes in the feed-forward block and the dropout rate\n",
    "    def __init__(self,  self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)]) # List of three Residual Connections with dropout rate\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "\n",
    "        # Self-Attention block with query, key, and value plus the target language mask\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "\n",
    "        # The Cross-Attention block using two 'encoder_ouput's for key and value plus the source language mask. It also takes in 'x' for Decoder queries\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "\n",
    "        # Feed-forward block with residual connections\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "O-Nti0ONygHZ"
   },
   "outputs": [],
   "source": [
    "# Building Decoder\n",
    "# A Decoder can have several Decoder Blocks\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    # The Decoder takes in instances of 'DecoderBlock'\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Storing the 'DecoderBlock's\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization() # Layer to normalize the output\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "\n",
    "        # Iterating over each DecoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            # Applies each DecoderBlock to the input 'x' plus the encoder output and source and target masks\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x) # Returns normalized output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQr_uHTRygHZ"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">You can see in the Decoder image that after running a stack of <code>DecoderBlock</code>s, we have a Linear Layer and a Softmax function to the output of probabilities. The <code>ProjectionLayer</code> class below is responsible for converting the output of the model into a probability distribution over the <i>vocabulary</i>, where we select each output token from a vocabulary of possible tokens.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "k0neuuiGygHZ"
   },
   "outputs": [],
   "source": [
    "# Buiding Linear Layer\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None: # Model dimension and the size of the output vocabulary\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size) # Linear layer for projecting the feature space of 'd_model' to the output space of 'vocab_size'\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim = -1) # Applying the log Softmax function to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHsfVIoPygHZ"
   },
   "source": [
    "<div id = 'building-transformer'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 32px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\">Building the Transformer</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuv8uZyVygHa"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We finally have every component of the Transformer architecture ready. We may now construct the Transformer by putting it all together.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>Transformer</code> class below, we will bring together all the components of the model's architecture.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tupTpoYUygHa"
   },
   "outputs": [],
   "source": [
    "# Creating the Transformer Architecture\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    # This takes in the encoder and decoder, as well the embeddings for the source and target language.\n",
    "    # It also takes in the Positional Encoding for the source and target language, as well as the projection layer\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    # Encoder\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src) # Applying source embeddings to the input source language\n",
    "        src = self.src_pos(src) # Applying source positional encoding to the source embeddings\n",
    "        return self.encoder(src, src_mask) # Returning the source embeddings plus a source mask to prevent attention to certain elements\n",
    "\n",
    "    # Decoder\n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt) # Applying target embeddings to the input target language (tgt)\n",
    "        tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n",
    "\n",
    "        # Returning the target embeddings, the output of the encoder, and both source and target masks\n",
    "        # The target mask ensures that the model won't 'see' future elements of the sequence\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "    # Applying Projection Layer with the Softmax function to the Decoder output\n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QZ_sGKhygHa"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The architecture is finally ready. We now define a function called <code>build_transformer</code>, in which we define the parameters and everything we need to have a fully operational Transformer model for the task of <b>machine translation</b>.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will set the same parameters as in the original paper, <a href = \"https://arxiv.org/pdf/1706.03762.pdf\"><i>Attention Is All You Need</i></a>, where $d_{model}$ = 512, $N$ = 6, $h$ = 8, dropout rate $P_{drop}$ = 0.1, and $d_{ff}$ = 2048.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wnGQZ1qwygHa"
   },
   "outputs": [],
   "source": [
    "# Building & Initializing Transformer\n",
    "\n",
    "# Definin function and its parameter, including model dimension, number of encoder and decoder stacks, heads, etc.\n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 5, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "\n",
    "    # Creating Embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size) # Source language (Source Vocabulary to 512-dimensional vectors)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size) # Target language (Target Vocabulary to 512-dimensional vectors)\n",
    "\n",
    "    # Creating Positional Encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # Positional encoding for the source language embeddings\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # Positional encoding for the target language embeddings\n",
    "\n",
    "    # Creating EncoderBlocks\n",
    "    encoder_blocks = [] # Initial list of empty EncoderBlocks\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "\n",
    "        # Combine layers into an EncoderBlock\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block) # Appending EncoderBlock to the list of EncoderBlocks\n",
    "\n",
    "    # Creating DecoderBlocks\n",
    "    decoder_blocks = [] # Initial list of empty DecoderBlocks\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Cross-Attention\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "\n",
    "        # Combining layers into a DecoderBlock\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block) # Appending DecoderBlock to the list of DecoderBlocks\n",
    "\n",
    "    # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Creating projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # Map the output of Decoder to the Target Vocabulary Space\n",
    "\n",
    "    # Creating the transformer by combining everything above\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer # Assembled and initialized Transformer. Ready to be trained and validated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSGzCr3WygHa"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The model is now ready to be trained!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4VRDnMIygHa"
   },
   "source": [
    "<div id = 'tokenizer'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\"><br><br>Tokenizer</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruD_aUuoygHa"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Tokenization is a crucial preprocessing step for our Transformer model. In this step, we convert raw text into a number format that the model can process.  </p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">There are several Tokenization strategies. We will use the <i>word-level tokenization</i> to transform each word in a sentence into a token.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9ljxoFUygHa"
   },
   "source": [
    "<center>\n",
    "    <img src = \"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5e749c-b0bd-4496-85a1-9b4397ad935f_1400x787.jpeg\" width = 800, height= 800>\n",
    "<p style = \"font-size: 16px;\n",
    "            font-family: 'Georgia', serif;\n",
    "            text-align: center;\n",
    "            margin-top: 10px;\">Different tokenization strategies. Source: <a href = \"https://shaankhosla.substack.com/p/talking-tokenization\">shaankhosla.substack.com</a>.</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fh_sstopygHa"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">After tokenizing a sentence, we map each token to an unique integer ID based on the created vocabulary present in the training corpus during the training of the tokenizer. Each integer number represents a specific word in the vocabulary.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Besides the words in the training corpus, Transformers use special tokens for specific purposes. These are some that we will define right away:</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>â€¢ [UNK]:</b> This token is used to identify an unknown word in the sequence.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>â€¢ [PAD]:</b> Padding token to ensure that all sequences in a batch have the same length, so we pad shorter sentences with this token. We use attention masks to <i>\"tell\"</i> the model to ignore the padded tokens during training since they don't have any real meaning to the task.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>â€¢  [SOS]:</b> This is a token used to signal the <i>Start of Sentence</i>.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>â€¢  [EOS]:</b> This is a token used to signal the <i>End of Sentence</i>.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>build_tokenizer</code> function below, we ensure a tokenizer is ready to train the model. It checks if there is an existing tokenizer, and if that is not the case, it trains a new tokenizer.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pi6IlwKKygHa"
   },
   "outputs": [],
   "source": [
    "# Defining Tokenizer\n",
    "def build_tokenizer(config, ds, lang):\n",
    "\n",
    "    # Crating a file path for the tokenizer\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "\n",
    "    # Checking if Tokenizer already exists\n",
    "    if not Path.exists(tokenizer_path):\n",
    "\n",
    "        # If it doesn't exist, we create a new one\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = 'unk')) # Initializing a new world-level tokenizer\n",
    "        tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n",
    "\n",
    "        # Creating a trainer for the new tokenizer\n",
    "        trainer = WordLevelTrainer(special_tokens = [\"unk\", \"pad\",\n",
    "                                                     \"sos\", \"eos\"]) # Defining Word Level strategy and special tokens\n",
    "\n",
    "        # Training new tokenizer on sentences from the dataset and language specified\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n",
    "        tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n",
    "    return tokenizer # Returns the loaded tokenizer or the trained tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIWF8g7lygHb"
   },
   "source": [
    "<div id = 'dataset'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\"><br><br>Loading Dataset</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtyg-9IlygHb"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">For this task, we will use the <a href = \"opus_books Â· Datasets at Hugging Face\">OpusBooks dataset</a>, available on ðŸ¤—Hugging Face. This dataset consists of two features, <code>id</code> and <code>translation</code>. The <code>translation</code> feature contains pairs of sentences in different languages, such as Spanish and Portuguese, English and French, and so forth.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">I first tried translating sentences from English to Portugueseâ€”my native tongue â€” but there are only 1.4k examples for this pair, so the results were not satisfying in the current configurations for this model. I then tried to use the English-French pair due to its higher number of examplesâ€”127kâ€”but it would take too long to train with the current configurations. I then opted to train the model on the English-Italian pair, the same one used in the <a href = \"https://youtu.be/ISNdQcPhsts?si=253J39cose6IdsLv\">Coding a Transformer from scratch on PyTorch, with full explanation, training and inference\n",
    "</a> video, as that was a good balance between performance and time of training.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We start by defining the <code>get_all_sentences</code> function to iterate over the dataset and extract the sentences according to the language pair definedâ€”we will do that later.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "abcXmFjvygHb"
   },
   "outputs": [],
   "source": [
    "# Iterating through dataset to extract the original sentence and its translation\n",
    "def get_all_sentences(ds, lang):\n",
    "    for pair in ds:\n",
    "        yield pair['translation'][lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAnWXzPqygHb"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>get_ds</code> function is defined to load and prepare the dataset for training and validation. In this function, we build or load the tokenizer, split the dataset, and create DataLoaders, so the model can successfully iterate over the dataset in batches. The result of these functions is tokenizers for the source and target languages plus the DataLoader objects.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcHvC9qYfCPq"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MMPdqwng0F2_"
   },
   "outputs": [],
   "source": [
    "dict_dataset = []\n",
    "trg = open(\"train_data/lij_train.txt\",encoding=\"utf-8\").readlines()\n",
    "source = open(\"train_data/eng_train.txt\",encoding=\"utf-8\").readlines()\n",
    "\n",
    "\n",
    "for i in range(len(source)):\n",
    "  dict_dataset.append({'id':i, 'translation': {'eng':source[i], 'lij':trg[i]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "tvXlZyECygHb"
   },
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "\n",
    "    # Loading the train portion of the OpusBooks dataset.\n",
    "    # The Language pairs will be defined in the 'config' dictionary we will build later\n",
    "    ds_raw = dict_dataset\n",
    "\n",
    "    # Building or loading tokenizer for both the source and target languages\n",
    "    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Splitting the dataset for training and validation\n",
    "    train_ds_size = int(0.95 * len(ds_raw)) # 90% for training\n",
    "    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n",
    "    # Processing data with the BilingualDataset class, which we will define below\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    for pair in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "    # Creating dataloaders for the training and validadion sets\n",
    "    # Dataloaders are used to iterate over the dataset in batches during training and validation\n",
    "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n",
    "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdBtgzJ-ygHb"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We define the <code>casual_mask</code> function to create a mask for the attention mechanism of the decoder. This mask prevents the model from having information about future elements in the sequence. </p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We start by making a square grid filled with ones. We determine the grid size with the <code>size</code> parameter. Then, we change all the numbers above the main diagonal line to zeros. Every number on one side becomes a zero, while the rest remain ones. The function then flips all these values, turning ones into zeros and zeros into ones. This process is crucial for models that predict future tokens in a sequence.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CVacmZxMygHb"
   },
   "outputs": [],
   "source": [
    "def casual_mask(size):\n",
    "        # Creating a square matrix of dimensions 'size x size' filled with ones\n",
    "        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "        return mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viGiLW_TygHc"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>BilingualDataset</code> class processes the texts of the target and source languages in the dataset by tokenizing them and adding all the necessary special tokens. This class also certifies that the sentences are within a maximum sequence length for both languages and pads all necessary sentences.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Mqt3vXh-ygHc"
   },
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n",
    "    # 'seq_len' defines the sequence length for both languages\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        # Defining special tokens by using the target language tokenizer\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"sos\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"eos\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"pad\")], dtype=torch.int64)\n",
    "        self.sentences_embedings = dict()\n",
    "        print('Converting sentences to word embeddings')\n",
    "        for i in tqdm(range(len(ds))):\n",
    "            tokenized_input = self.tokenizer_src.encode(ds[i]['translation'][src_lang]).tokens\n",
    "            enc_num_padding_tokens = self.seq_len - len(tokenized_input) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
    "\n",
    "            # Building the encoder input tensor by combining several elements\n",
    "            encoder_input = ['sos'] + tokenized_input + ['eos'] + ['pad'] * enc_num_padding_tokens\n",
    "            self.sentences_embedings[i] = (torch.tensor([word2vec_model[j.lower()] if word2vec_model.has_index_for(j.lower()) else word2vec_model['unk'] for j in encoder_input]),torch.tensor([1 if l != 'pad' else 0 for l in encoder_input]).unsqueeze(0).unsqueeze(0).int())\n",
    "    \n",
    "\n",
    "    # Total number of instances in the dataset (some pairs are larger than others)\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    # Using the index to retrive source and target texts\n",
    "    def __getitem__(self, index: any) -> any:\n",
    "        src_target_pair = self.ds[index]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Tokenizing target texts\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Computing how many padding tokens need to be added to the tokenized texts\n",
    "        # Target tokens\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n",
    "\n",
    "        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n",
    "        # given the current sequence length limit (this will be defined in the config dictionary below)\n",
    "        if dec_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long')\n",
    "\n",
    "        # Building the encoder input tensor by combining several elements\n",
    "        encoder_input = self.sentences_embedings[index][0]\n",
    "        encoder_mask = self.sentences_embedings[index][1]\n",
    "\n",
    "        # Building the decoder input tensor by combining several elements\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token, # inserting the '[SOS]' token\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "\n",
    "        )\n",
    "\n",
    "        # Creating a label tensor, the expected output for training the model\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                self.eos_token, # Inserting the '[EOS]' token\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n",
    "\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n",
    "        # assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input,\n",
    "            'encoder_mask': encoder_mask,\n",
    "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
    "            'label': label,\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "196BNkOgygHd"
   },
   "source": [
    "<div id = 'val'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\"><br><br>Validation Loop</div>\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ap_GBEAsygHd"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will now create two functions for the validation loop. The validation loop is crucial to evaluate model performance in translating sentences from data it has not seen during training.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will define two functions. The first function, <code>greedy_decode</code>, gives us the model's output by obtaining the most probable next token. The second function, <code>run_validation</code>, is responsible for running the validation process in which we decode the model's output and compare it with the reference text for the target sentence.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6jgHXn9wygHd"
   },
   "outputs": [],
   "source": [
    "# Define function to obtain the most probable next token\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    # Retrieving the indices from the start and end of sequences of the target tokens\n",
    "    sos_idx = tokenizer_tgt.token_to_id('sos')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('eos')\n",
    "\n",
    "    # Computing the output of the encoder for the source sequence\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initializing the decoder input with the Start of Sentence token\n",
    "    decoder_input = torch.empty(1,1).fill_(sos_idx).type('torch.LongTensor').to(device)\n",
    "\n",
    "    # Looping until the 'max_len', maximum length, is reached\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # Building a mask for the decoder input\n",
    "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # Calculating the output of the decoder\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # Applying the projection layer to get the probabilities for the next token\n",
    "        prob = model.project(out[:, -1])\n",
    "\n",
    "        # Selecting token with the highest probability\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1).type('torch.LongTensor').fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "        # If the next token is an End of Sentence token, we finish the loop\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "JB7S2Pr7ygHd"
   },
   "outputs": [],
   "source": [
    "# Defining function to evaluate the model on the validation dataset\n",
    "# num_examples = 2, two examples per run\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
    "    model.eval() # Setting model to evaluation mode\n",
    "    count = 0 # Initializing counter to keep track of how many examples have been processed\n",
    "\n",
    "    console_width = 80 # Fixed witdh for printed messages\n",
    "\n",
    "    # Creating evaluation loop\n",
    "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "\n",
    "            # Ensuring that the batch_size of the validation set is 1\n",
    "            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
    "\n",
    "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            # Retrieving source and target texts from the batch\n",
    "            source_text = batch['src_text'][0]\n",
    "            target_text = batch['tgt_text'][0] # True translation\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
    "\n",
    "            # Printing results\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f'SOURCE: {source_text}')\n",
    "            print_msg(f'TARGET: {target_text}')\n",
    "            print_msg(f'PREDICTED: {model_out_text}')\n",
    "\n",
    "            # After two examples, we break the loop\n",
    "            if count == num_examples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t81w8x3yygHd"
   },
   "source": [
    "<div id = 'train'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\"><br><br>Training Loop</div>\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3m6wiwtygHd"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We are ready to train our Transformer model on the OpusBook dataset for the English to Italian translation task.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We first start by defining the <code>get_model</code> function to load the model by calling the <code>build_transformer</code> function we have previously defined. This function uses the <code>config</code> dictionary to set a few parameters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "owZlmOX7ygHd"
   },
   "outputs": [],
   "source": [
    "# We pass as parameters the config dictionary, the length of the vocabylary of the source language and the target language\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "\n",
    "    # Loading model using the 'build_transformer' function.\n",
    "    # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aj0y79HygHd"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">I have mentioned the <code>config</code> dictionary several times throughout this notebook. Now, it is time to create it.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the following cell, we will define two functions to configure our model and the training process.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>get_config</code> function, we define crucial parameters for the training process. <code>batch_size</code> for the number of training examples used in one iteration, <code>num_epochs</code> as the number of times the entire dataset is passed forward and backward through the Transformer, <code>lr</code> as the learning rate for the optimizer, etc. We will also finally define the pairs from the OpusBook dataset, <code>'lang_src': 'en'</code> for selecting English as the source language and <code>'lang_tgt': 'it'</code> for selecting Italian as the target language.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>get_weights_file_path</code> function constructs the file path for saving or loading model weights for any specific epoch.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ieycP0m2ygHd"
   },
   "outputs": [],
   "source": [
    "# Define settings for building and training the transformer model\n",
    "def get_config():\n",
    "    return{\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 81,\n",
    "        'lr': 10**-4,\n",
    "        'seq_len': 115,\n",
    "        'd_model': 100, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n",
    "        'lang_src': 'eng',\n",
    "        'lang_tgt': 'lij',\n",
    "        'model_folder': 'weights',\n",
    "        'model_basename': 'tmodel_',\n",
    "        'preload': None,\n",
    "        'tokenizer_file': 'tokenizer_{0}.json',\n",
    "        'experiment_name': 'runs/tmodel'\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to construct the path for saving and retrieving model weights\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = config['model_folder'] # Extracting model folder from the config\n",
    "    model_basename = config['model_basename'] # Extracting the base name for model files\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n",
    "    return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsiZ3TL8ygHd"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We finally define our last function, <code>train_model</code>, which takes the <code>config</code> arguments as input. </p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In this function, we will set everything up for the training. We will load the model and its necessary components onto the GPU for faster training, set the <code>Adam</code> optimizer, and configure the <code>CrossEntropyLoss</code> function to compute the differences between the translations output by the model and the reference translations from the dataset. </p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Every loop necessary for iterating over the training batches, performing backpropagation, and computing the gradients is in this function. We will also use it to run the validation function and save the current state of the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "F1x1VgMbqnsQ"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer):\n",
    "    model.eval() # Setting model to evaluation mode\n",
    "    count = 0\n",
    "    refs = []\n",
    "    hyps = []\n",
    "    # Creating evaluation loop\n",
    "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
    "        for batch in tqdm(validation_ds):\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            count += 1\n",
    "\n",
    "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            # Retrieving source and target texts from the batch\n",
    "            source_text = batch['src_text'][0]\n",
    "            target_text = batch['tgt_text'][0] # True translation\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
    "\n",
    "            refs.append(target_text)\n",
    "            hyps.append(model_out_text)\n",
    "    print(sc.corpus_bleu(references=[refs], hypotheses=hyps))\n",
    "    print(sc.corpus_chrf(references=[refs], hypotheses=hyps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "mT_SjuxbqnsQ"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "W_J_oIAdygHe"
   },
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Setting up device to run on GPU to train faster\n",
    "    print(f\"Using device {device}\")\n",
    "\n",
    "    # Creating model directory to store weights\n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "\n",
    "    # Initializing model on the GPU using the 'get_model' function\n",
    "    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    # Setting up the Adam optimizer with the specified learning rate from the '\n",
    "    # config' dictionary plus an epsilon value\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "\n",
    "    # Initializing epoch and global step variables\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "    # Checking if there is a pre-trained model to load\n",
    "    # If true, loads it\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename) # Loading model\n",
    "\n",
    "        # Sets epoch to the saved in the state plus one, to resume from where it stopped\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        # Loading the optimizer state from the saved model\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        # Loading the global step state from the saved model\n",
    "        global_step = state['global_step']\n",
    "\n",
    "    # Initializing CrossEntropyLoss function for training\n",
    "    # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n",
    "    # We also apply label_smoothing to prevent overfitting\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('pad'), label_smoothing = 0.1).to(device)\n",
    "\n",
    "    # Initializing training loop\n",
    "\n",
    "    # Iterating over each epoch from the 'initial_epoch' variable up to\n",
    "    # the number of epochs informed in the config\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "\n",
    "        # Initializing an iterator over the training dataloader\n",
    "        # We also use tqdm to display a progress bar\n",
    "        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n",
    "\n",
    "        # For each batch...\n",
    "        for batch in batch_iterator:\n",
    "            model.train() # Train the model\n",
    "\n",
    "            # Loading input data and masks onto the GPU\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)\n",
    "\n",
    "            # Running tensors through the Transformer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "\n",
    "            # Loading the target labels onto the GPU\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Computing loss between model's output and true labels\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "\n",
    "            # Updating progress bar\n",
    "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Performing backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters based on the gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clearing the gradients to prepare for the next batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step += 1 # Updating global step count\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # We run the 'run_validation' function at the end of each epoch\n",
    "        # to evaluate model performance\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        if epoch != 0 and (epoch == 50 or epoch == 70):\n",
    "            calculate_metrics(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "        # Saving model\n",
    "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        # Writting current model state to the 'model_filename'\n",
    "        torch.save({\n",
    "            'epoch': epoch, # Current epoch\n",
    "            'model_state_dict': model.state_dict(),# Current model state\n",
    "            'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n",
    "            'global_step': global_step # Current global step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3-elu2vygHe"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We can now train the model!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "-djccianygHe",
    "outputId": "1a131963-8c93-4255-e7c4-1242beda579b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Converting sentences to word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9827/9827 [00:10<00:00, 913.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting sentences to word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 518/518 [00:00<00:00, 914.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 84\n",
      "Max length of target sentence: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 00: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:40<00:00, 15.03it/s, loss=7.852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: wait for me at the bus stop bus\n",
      "\n",
      "TARGET: aspetime a-a fermÃ¢ de lâ€™autobo\n",
      "\n",
      "PREDICTED: o o o o o o o o o o o o l\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: a rhetoric really heavy\n",
      "\n",
      "TARGET: unna retÃ²rica davei ascidiosa\n",
      "\n",
      "PREDICTED: o o o o o l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 01: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:41<00:00, 14.96it/s, loss=6.648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"Only in this generalized sense does it become possible to speak of a \"\"medieval civilization\"\", which in Elias's sense would have been an oxymoron.\"\n",
      "\n",
      "TARGET: Solo inte sto senso generale se peu parlÃ¢ de unna Â«Ã§iviltÃ¦ de l'etÃ¦ de mezoÂ», che into senso do Elias a saieiva stÃ¦to un oscimoro.\n",
      "\n",
      "PREDICTED: o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© l ' Ã© l â€™ Ã© l ' Ã© l ' Ã© o l â€™ Ã© o l â€™ Ã© l â€™ Ã© l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o l â€™ Ã©\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: here we talk about ligure\n",
      "\n",
      "TARGET: chÃ¬ parlemmo ligure\n",
      "\n",
      "PREDICTED: o l â€™ Ã© o l â€™ Ã©\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 02: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:41<00:00, 14.91it/s, loss=5.954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I bought the dough for tomorrow\n",
      "\n",
      "TARGET: Ã² accattou i fidÃª pe doman\n",
      "\n",
      "PREDICTED: o l â€™ Ã© o l â€™ Ã©\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The first observed archaea were extremophiles, living in extreme environments, such as hot springs and salt lakes with no other organisms.\n",
      "\n",
      "TARGET: I primmi archei stÃ¦ti osservÃ¦ ean estremÃ²fili, che viveivan inte di ambienti estremi, comme vivagne cade e laghi salÃ¦ sensa di atri organismi.\n",
      "\n",
      "PREDICTED: O l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© de l â€™ Ã© de l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© de l â€™ Ã© de l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - o l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 03: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:40<00:00, 15.01it/s, loss=5.719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: There is also potential to use the silk producing machinery to make other valuable proteins.\n",
      "\n",
      "TARGET: Ghâ€™Ã© ascÃ¬ o potenÃ§iale pe deuviÃ¢ i machinÃ¤i pe-a produÃ§ion da sÃ¦a pe produe de atre proteiÃ±e de valÃ´.\n",
      "\n",
      "PREDICTED: A - o l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - o l â€™ Ã© a - a - a - a - o l â€™ Ã© a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a -\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: is a boy of a little judgment\n",
      "\n",
      "TARGET: o lâ€™Ã© un figgeu de pÃ¶co sÃ¦ximo\n",
      "\n",
      "PREDICTED: o l â€™ Ã© un sce l â€™ Ã© un sce l â€™ Ã© un sce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 04: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:41<00:00, 14.84it/s, loss=5.555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: ve have already said?\n",
      "\n",
      "TARGET: ve lâ€™an za dito?\n",
      "\n",
      "PREDICTED: ti no no no no no no no no ?\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: I bought the dough for tomorrow\n",
      "\n",
      "TARGET: Ã² accattou i fidÃª pe doman\n",
      "\n",
      "PREDICTED: o l â€™ Ã  ti ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 05: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:41<00:00, 14.79it/s, loss=6.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Nonetheless, Earth is the only place in the universe humans know to harbor life.\n",
      "\n",
      "TARGET: Sciben, a TÃ¦ra a lâ€™Ã© lâ€™unico pÃ²sto inte lâ€™universo donde i Ã²mmi saccian che ghe segge da vitta.\n",
      "\n",
      "PREDICTED: O l â€™ Ã© stÃ¦to stÃ¦to o l â€™ Ã© stÃ¦to stÃ¦to stÃ¦to stÃ¦to o l â€™ Ã© stÃ¦to stÃ¦to stÃ¦to stÃ¦to stÃ¦to stÃ¦to a - o l â€™ Ã© stÃ¦to stÃ¦to stÃ¦to stÃ¦to stÃ¦to stÃ¦to un sce .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: cut short the talk\n",
      "\n",
      "TARGET: taggiÃ¢ curto o discorso\n",
      "\n",
      "PREDICTED: i mÃ²ddi de l â€™ Ã© stÃ¦to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 06: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:41<00:00, 14.99it/s, loss=4.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: In his book Sensory Ecology biophysicist David B. Dusenbery called these causal inputs.\n",
      "\n",
      "TARGET: Into seu libbro Sensory Ecology, o biofixico David B. Dusenbery o l'Ã  ciammou sti chÃ¬ input causÃ¦.\n",
      "\n",
      "PREDICTED: O l â€™ Ã  fÃ¦to o l â€™ Ã  fÃ¦to de l â€™ Ã  fÃ¦to che o l â€™ Ã  fÃ¦to de l â€™ Ã  fÃ¦to de l â€™ Ã  fÃ¦to .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: \"A World Map on a Regular Icosahedron by Gnomonic Projection.\"\"\"\n",
      "\n",
      "TARGET: Â«Unna mappa do mondo in sce unnâ€™icosaedro regolare con proieÃ§ion gnomÃ²nica.Â»\n",
      "\n",
      "PREDICTED: A - o l â€™ Ã© un sciÃ¢ seu seu seu seu seu seu seu seu seu seu .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 07: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:40<00:00, 15.07it/s, loss=5.594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: that mason works roughly\n",
      "\n",
      "TARGET: quello massacan o travaggia a-a biscÃ¶chiÃ±a\n",
      "\n",
      "PREDICTED: o l â€™ Ã© stÃ¦to che no l â€™ Ã© stÃ¦to\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: in the morning the ice is melted\n",
      "\n",
      "TARGET: a-a mattin a giassa a sâ€™Ã© deslenguÃ¢\n",
      "\n",
      "PREDICTED: o l â€™ Ã© stÃ¦to o mÃ¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 08: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:41<00:00, 14.94it/s, loss=5.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The time variation is discussed more fully in the article on axial tilt.\n",
      "\n",
      "TARGET: In sciÃ¢ variaÃ§ion temporale vÃ«gne dÃ¦to unna trattaÃ§ion ciÃ¹ approfondia inte lâ€™articolo in sciÃ¢ pendensa assiale.\n",
      "\n",
      "PREDICTED: A - a teoria a l â€™ Ã© a - a teoria de l â€™ Ã© a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a - a -\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: In the 1970s, the term was used as a way of linking the experiences, issues, and struggles of groups of colonized people across international borders.\n",
      "\n",
      "TARGET: Di anni '70, o termine o l'Ã© stÃ¦to addeuviou comme unna mainea pe collegÃ¢ e esperiense, i problemi e e battagge de di gruppi de persoÃ±e colonizzÃ¦ a-o de lÃ  di confin internaÃ§ionali.\n",
      "\n",
      "PREDICTED: A - o secolo , a l ' Ã© stÃ¦to unna seu seu ciÃ¹ de l ' Ã© stÃ¦to de l ' Ã© stÃ¦to de l ' Ã© stÃ¦to de seu seu seu Unii .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 09: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:41<00:00, 14.84it/s, loss=5.400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: is coming immediately to tell us all\n",
      "\n",
      "TARGET: a lâ€™Ã© vegnua a-a primma Ã  dÃ®ne tutto\n",
      "\n",
      "PREDICTED: o l â€™ Ã© stÃ¦to fÃ¦to de fÃ¢\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The pre-Socratic philosophers shared the intuition that there was a single explanation that could explain both the plurality and the singularity of the whole â€“ and that explanation would not be direct actions of the gods.\n",
      "\n",
      "TARGET: I filÃ²sofi presocratichi condividdeivan lâ€™intuiÃ§ion che ghâ€™ea unna sola spiegaÃ§ion châ€™a poeiva spiegÃ¢ tanto a pluralitÃ¦ comme a scingolaritÃ¦ do tutto: e quella spiegaÃ§ion a no saieiva stÃ¦ta de aÃ§ioin dirette de divinitÃ¦.\n",
      "\n",
      "PREDICTED: O l â€™ Ã© stÃ¦to stÃ¦to o scistema de l â€™ Ã© stÃ¦to fÃ¦to che o l â€™ Ã© stÃ¦to fÃ¦to che i numeri , e o l â€™ Ã© stÃ¦to un atro , e o l â€™ Ã© stÃ¦to fÃ¦to che i numeri e o l â€™ Ã© stÃ¦to fÃ¦to de l â€™ Ã© stÃ¦to un atro .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:42<00:00, 14.37it/s, loss=4.826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Nanotechnology, also shortened to nanotech, is the use of matter on an atomic, molecular, and supramolecular scale for industrial purposes.\n",
      "\n",
      "TARGET: E nanotecnologie (Â«nanotechÂ» inta seu abbreviaÃ§ion ingleise) representan lâ€™utilizzaÃ§ion da matÃ«ia in sce scÃ¢ atÃ²mica, molecolare Ã² sorviamolecolare pe finalitÃ¦ dâ€™industria.\n",
      "\n",
      "PREDICTED: Pe - o scistema , o l â€™ Ã© stÃ¦to un exempio , o l â€™ Ã© un scistema de l â€™ Ã© o scistema de l â€™ Ã© un scistema de l â€™ Ã© stÃ¦to un scistema de l â€™ Ã¦gua , e o l â€™ Ã© un scistema de l â€™ Ã© un scistema .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: In contrast to the Imago Mundi, an earlier Babylonian world map dating back to the 9th century BC depicted Babylon as being further north from the center of the world, though it is not certain what that center was supposed to represent.\n",
      "\n",
      "TARGET: In contrasto con lâ€™Imago Mundi, un mappamondo babiloneise ciÃ¹ antigo châ€™o remontava a-o secolo IX a.C. o representava BabilÃ²nia ciÃ¹ a-o nÃ²rd do Ã§entro do mondo, ascÃ¬ ben che no lâ€™Ã© seguo cÃ¶sâ€™o representesse quello Ã§entro.\n",
      "\n",
      "PREDICTED: O primmo , o l â€™ Ã© stÃ¦to stÃ¦to stÃ¦to stÃ¦to stÃ¦to un scistema de l â€™ Ã© stÃ¦to fÃ¦to che o secolo , ch â€™ o l â€™ Ã© stÃ¦to o l â€™ Ã© stÃ¦to o secolo , ch â€™ o l â€™ Ã© stÃ¦to fÃ¦to che o l â€™ Ã© stÃ¦to o l â€™ Ã© stÃ¦to stÃ¦to fÃ¦to che o l â€™ Ã© stÃ¦to stÃ¦to stÃ¦to stÃ¦to stÃ¦to stÃ¦to fÃ¦to da - o l â€™ Ã© stÃ¦to stÃ¦to stÃ¦to stÃ¦to stÃ¦to stÃ¦to stÃ¦to stÃ¦to o seu vÃ²tta da - o primmo o primmo o primmo Ã«se stÃ¦to stÃ¦to stÃ¦to stÃ¦to o primmo de l â€™ Ã© o seu parte da - o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:41<00:00, 14.68it/s, loss=5.403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Small scale refers to world maps or maps of large regions such as continents or large nations.\n",
      "\n",
      "TARGET: Unna scÃ¢ picciÃ±a a fa referensa a-e mappe do mondo Ã² Ã  de mappe de regioin assÃ¦ esteise, comme continenti Ã² grende naÃ§ioin.\n",
      "\n",
      "PREDICTED: I atri gruppi en deuviÃ¦ pe - i atri gruppi de spesso Ã² di atri gruppi Ã² de spesso Ã² de spesso Ã² de spesso .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: are the two of the morning\n",
      "\n",
      "TARGET: lâ€™Ã© doe oe da mattin\n",
      "\n",
      "PREDICTED: e doÃ® son in sciÃ¢ - a - a mÃ¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:42<00:00, 14.57it/s, loss=4.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The Western empire would fall, in 476 CE, to German influence under Odoacer.\n",
      "\n",
      "TARGET: L'ImpÃ«io de ponente o l'Ã© cheito, do 476 d.C., pe l'influensa germanica sotta Ã  Odoacre.\n",
      "\n",
      "PREDICTED: O secolo o l ' Ã© stÃ¦to stÃ¦to fÃ¦to pe - o secolo , o secolo , o l ' Ã  l ' EtÃ¦ de l ' EtÃ¦ de l ' EtÃ¦ de l ' EtÃ¦ de l ' EtÃ¦ de l ' EtÃ¦ de l ' EtÃ¦ de l ' EtÃ¦ de l ' EtÃ¦ de l ' EurÃ¶pa .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The set of all integral linear combinations of a and b is actually the same as the set of all multiples of g (mg, where m is an integer).\n",
      "\n",
      "TARGET: Lâ€™insemme de tutte e combinaÃ§ioin lineare integrale de a e b o lâ€™Ã©, in realtÃ¦, o mÃ¦ximo che lâ€™insemme de tutti i moltipli de g (mg, donde m o lâ€™Ã© un intrego).\n",
      "\n",
      "PREDICTED: A - o numero de un exempio , o l â€™ Ã© o numero de un numero de un numero de un numero ( o l â€™ Ã© o l â€™ Ã© o l â€™ Ã© o numero de un numero ).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:42<00:00, 14.64it/s, loss=4.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: The Occupational Safety and Health Administration (OSHA) in the United States, recognizing the unique characteristics of the laboratory workplace, has tailored a standard for occupational exposure to hazardous chemicals in laboratories.\n",
      "\n",
      "TARGET: A Occupational Safety and Health Administration (OSHA) di Stati Unii, con reconosce e caratteristiche uniche do pÃ²sto de travaggio di laboratÃ¶io, o l'Ã  definio un standard pe l'espoxiÃ§ion profescionale Ã  de sostanse chimiche de reisego inti laboratÃ¶i.\n",
      "\n",
      "PREDICTED: A teoria de l ' antropologia ( e a l ' Ã  l ' Ã¦gua ( a - a teoria de l ' Ã  l ' Ã¦gua ( che i atri atri atri atri gruppi de l ' Ã¦gua , pe - o sviluppo de l ' Ã  l ' Ã¦gua .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: that poor meschinetto crying\n",
      "\n",
      "TARGET: quello pÃ¶veo meschinetto o cianzeiva\n",
      "\n",
      "PREDICTED: che o l â€™ Ã  fÃ¦to de sÃ²\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:42<00:00, 14.49it/s, loss=5.180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: This research provides scientific information and theories for the explanation of the nature and the properties of the world.\n",
      "\n",
      "TARGET: Sta reÃ§erca a fornisce informaÃ§ion scientifica e teorie pe l'ascciÃ¦imento da natua e e caratteristiche do mondo.\n",
      "\n",
      "PREDICTED: Sto chÃ¬ o l ' Ã© o studio de l ' ambiente e o l ' Ã© o studio de l ' ambiente e o sviluppo de l ' ambiente .\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: In partial response to Gill's statement, Professor of Biological Anthropology C. Loring Brace argues that the reason laymen and biological anthropologists can determine the geographic ancestry of an individual can be explained by the fact that biological characteristics are clinally distributed across the planet, and that does not translate into the concept of race.\n",
      "\n",
      "TARGET: Pe responde in parte a-a deciaraÃ§ion do Gill, o professÃ´ d'antropologia biolÃ²gica C. Loring Brace o sostÃ«gne che a raxon pe-a quÃ¦ di inesperti e bioantropÃ²logi arriÃ«scian Ã  determinÃ¢ l'ascendensa geografica de un individuo a peu Ã«se ascciÃ¦ia da-o fÃ¦to che e caratteristiche biolÃ²giche an unna distribuÃ§ion clinale into pianeta, e sto chÃ¬ o no se traduxe into conÃ§etto de razza.\n",
      "\n",
      "PREDICTED: Inte l ' idea de l ' antropologia a l ' Ã© a teoria de l ' Ã© a teoria de l ' evoluÃ§ion de l ' idea de l ' evoluÃ§ion , ma a peu Ã«se a - a teoria de l ' Ã© a teoria de l ' Ã© a teoria de l ' Ã© a teoria de l ' Ã© a - a teoria de l ' Ã© a - a teoria de l ' Ã© a teoria de l ' idea da - a - a seu mainea che no peuan Ã«se a teoria da - a teoria de l ' Ã© a - a teoria da - a teoria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 15:  17%|â–ˆâ–‹        | 104/615 [00:07<00:34, 14.75it/s, loss=4.894]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Filtering warnings\u001b[39;00m\n\u001b[0;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config() \u001b[38;5;66;03m# Retrieving config settings\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 83\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     80\u001b[0m writer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Performing backpropagation\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Updating parameters based on the gradients\u001b[39;00m\n\u001b[0;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\.conda\\envs\\cude_env\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\cude_env\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings('ignore') # Filtering warnings\n",
    "    config = get_config() # Retrieving config settings\n",
    "    train_model(config) # Training model with the config arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz3hJUnYqnsR"
   },
   "source": [
    "calculate_blue(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHuM2Ku3ygHe"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">As you can see below, we trained for 20 epochs, and the model has been slowly improving. The last epoch had the best performance, at 2.094. Training for more epochs, as well as fine-tuning some parameters, could lead to more promising results.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0vfNYaLygHe"
   },
   "source": [
    "<div id = 'conclusion'\n",
    "     style=\"font-family: 'Helvetica Neue', Arial, sans-serif;; text-align: left;\">\n",
    "    <div style=\"font-size: 38px; letter-spacing: 4.25px;color: #121212;\n",
    "                text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\n",
    "                border-bottom: 1.25px solid #121212\"><br><br>Conclusion</div>\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpG7OU1MygHe"
   },
   "source": [
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In this notebook, we have explored the original Transformer architecture in depth, as presented in the <i>Attention Is All You Need</i> research paper. We used PyTorch to implement it step-by-step on a language translation task using the OpusBook dataset for English-to-Italian translation. </p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The Transformer is a revolutionary step towards the most advanced models we have today, such as OpenAI's GPT-4 model. And that is why it is so relevant to comprehend how this architecture works and what it can achieve.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The resources behind this notebook are the paper <a href = \"https://arxiv.org/pdf/1706.03762.pdf\"><i><b>\"Attention Is All You Need\"</b></i></a> and the YouTube video <a href = \"https://www.youtube.com/watch?v=ISNdQcPhsts&t=9595s\"><b>Coding a Transformer from scratch on PyTorch, with full explanation, training and inference</b></a> posted by <a href = \"https://www.youtube.com/@umarjamilai\"><b>Umar Jamil</b></a>. I highly suggest you check both materials for a deeper understanding of the Transformer. </p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">If you liked the content of this notebook, feel free to leave an upvote and share it with friends and colleagues. I am also eager to read your comments, suggestions, and opinions.</p>\n",
    "\n",
    "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Thank you very much!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YE4ohzgIygHe"
   },
   "source": [
    "<hr style=\"border: 0;\n",
    "           height: 1px;\n",
    "           border-top: 0.85px;\n",
    "           solid #b2b2b2\">\n",
    "           \n",
    "<div style=\"text-align: left;\n",
    "            color: #8d8d8d;\n",
    "            padding-left: 15px;\n",
    "            font-size: 14.25px;\">\n",
    "    Luis Fernando Torres, 2024<br><br>\n",
    "    Let's connect!ðŸ”—<br>\n",
    "    <a href=\"https://www.linkedin.com/in/luuisotorres/\">LinkedIn</a> â€¢ <a href=\"https://medium.com/@luuisotorres\">Medium</a> â€¢ <a href = \"https://huggingface.co/luisotorres\">Hugging Face</a><br><br>\n",
    "</div>\n",
    "<div style=\"text-align: center;\n",
    "            margin-top: 50px;\n",
    "            color: #8d8d8d;\n",
    "            padding-left: 15px;\n",
    "            font-size: 14.25px;\"><b>Like my content? Feel free to <a href=\"https://www.buymeacoffee.com/luuisotorres\">Buy Me a Coffee â˜•</a></b>\n",
    "</div>\n",
    "<div style=\"text-align: center;\n",
    "            margin-top: 80px;\n",
    "            color: #8d8d8d;\n",
    "            padding-left: 15px;\n",
    "            font-size: 14.25px;\"><b>  <a href = \"https://luuisotorres.github.io/\">https://luuisotorres.github.io/</a> </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9-F-eGFeMfW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMnJ550tD37b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0aScnlRD3mW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNH8OuKkqnsS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryomyRHaqnsS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bs4FJUNtqnsS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y19QIuxMqnsS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQ7yUnOzqnsS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWm97oSCqnsS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
